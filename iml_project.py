# -*- coding: utf-8 -*-
"""iml_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kIj2rSf1vmmkPb9dEOxU1WuM_JPPpxzw
"""

from google.colab import drive
drive.mount('/content/drive')

"""Importing Libraries"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns

from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV, GridSearchCV, train_test_split
from scipy.stats import loguniform
from xgboost import XGBClassifier

"""Basic Preprocessing"""

df=pd.read_csv("/content/drive/MyDrive/Student_performance_data _.csv")

print("First few rows of the dataset:")
print(df.head())

print("\nDataset Information:")
print(df.info())

print("\nStatistical Summary of the dataset:")
print(df.describe())


# identify all the numerical and categorical columns

num_cols=df.select_dtypes(include=["float64","int64"]).columns
cat_cols=df.select_dtypes(include=["object"]).columns
if cat_cols.empty:
  print("No categorical columns")
else:
  print("categorical columns are: ", cat_cols)


print("numerical columns are: ", num_cols)


#checking for null values:
print("missing values in each column are:\n ", df.isnull().sum())
if (df.isnull().sum()==0).all():
  print("No missing values")

else:
  df=df.fillna(df.mean())

"""Plots for better understanding(preprocessing)"""

# plot histograms for numerical features

df.hist(figsize=(20,10),bins=7, color='lightblue')

numerical_features = df.select_dtypes(include=['float64', 'int64']).columns
# correlation heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(df[numerical_features].corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()


# Exclude 'StudentID' since it's an identifier
columns_to_plot = df.columns.drop('StudentID')

# Set up the size of the figure to accommodate all columns
plt.figure(figsize=(20, 30))

# Loop through all columns and create subplots for each
for i, column in enumerate(columns_to_plot, 1):
    plt.subplot(5, 3, i)
    sns.boxplot(y=df[column])
    plt.title(f'Boxplot of {column}')

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()

"""Note: here grade class classification is given as
GradeClass: Classification of students' grades based on GPA:

0: 'A' (GPA >= 3.5)

1: 'B' (3.0 <= GPA < 3.5)

2: 'C' (2.5 <= GPA < 3.0)

3: 'D' (2.0 <= GPA < 2.5)

4: 'F' (GPA < 2.0)

Hence the we can see inverese (negative) realtion between GradeClass and GPA, but in actual it is positve 0.78
Similarly Absences and GradeCLass have correlation of -0.73
"""

#scaling the dataset using standard scaler
scaler = StandardScaler()
new_df = df
numeric_columns = ['Age', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'ParentalSupport', 'GPA'] # Changed to a single list
new_df[numeric_columns] = scaler.fit_transform(new_df[numeric_columns])
print(new_df.head())

# Dropping the columns which have minimal correlation with GradeCLass
# Splitting into 80-20 train-test ratio

Y = new_df['GradeClass' ]
X = new_df.drop(columns=['GradeClass' , 'StudentID', 'Volunteering', 'Sports', 'Music'] , axis=1)
new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
print(new_X_train.shape)
print(new_X_test.shape)
print(new_y_train.shape)
print(new_y_test.shape)

"""Feature Importance"""

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestRegressor(random_state=42)
clf = clf.fit(X, Y)

fimp = pd.Series(data=clf.feature_importances_, index=X.columns).sort_values(ascending=False)

plt.figure(figsize=(17,13))
plt.title("Feature importance")
ax = sns.barplot(y=fimp.index, x=fimp.values, orient='h',palette="Set1")

#Checking that the changes have been made
new_X_train.head()

new_y_train.head()

"""Model accuracies"""

#Training the classification models and printing the accuracies
classification_models = {
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Support Vector Machine": SVC(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "Gaussian Naive Bayes": GaussianNB(),
    "XGBoost": XGBClassifier(),
}

model_names = []
accuracies = []

# Train and evaluate each model
for name, clf in classification_models.items():
    clf.fit(new_X_train, new_y_train)
    y_pred = clf.predict(new_X_test)
    score = accuracy_score(new_y_test, y_pred)
    model_names.append(name)
    accuracies.append(score)
    print(f"{name} accuracy: {score:.2f}")

# Create a DataFrame for model accuracies
new_df_models = pd.DataFrame({'Model': model_names, 'Accuracy': accuracies})

# Display the model accuracies
print(new_df_models)

import plotly.express as px

# Convert accuracies to percentages (with 2 decimal places)
new_df_models['Accuracy (%)'] = (new_df_models['Accuracy'] * 100).round(2)

# Plot model accuracies using Plotly and show the accuracy on top of each bar
fig = px.bar(new_df_models, x='Model', y='Accuracy', title='Model Accuracies',
             text='Accuracy (%)')

# Customize the text position and format
fig.update_traces(textposition='outside')

# Update the y-axis to show percentage
fig.update_layout(yaxis_tickformat='%')

# Show the plot
fig.show()

from sklearn.decomposition import PCA

# Fit PCA to the training data (for visualization purposes only)
pca = PCA().fit(new_X_train)

# Explained variance ratio for each component
explained_variance = pca.explained_variance_ratio_
cumulative_variance = explained_variance.cumsum()

# Generate colors for each component using a colormap
colors = plt.cm.viridis(np.linspace(0, 1, len(explained_variance)))

# Create a side-by-side plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Plot explained variance ratio for each component with multiple colors
ax1.bar(range(1, len(explained_variance) + 1), explained_variance, color=colors, alpha=0.7)
ax1.set_xlabel('Principal Components')
ax1.set_ylabel('Explained Variance Ratio')
ax1.set_title('Explained Variance by Each Component')

# Plot cumulative explained variance
ax2.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', color='purple')
ax2.fill_between(range(1, len(cumulative_variance) + 1), 0, cumulative_variance, color='lightblue', alpha=0.4)
ax2.set_xlabel('Number of Principal Components')
ax2.set_ylabel('Cumulative Explained Variance')
ax2.set_title('Cumulative Explained Variance')

# Show the plots
plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
import plotly.express as px
import pandas as pd

# Apply PCA after standard scaling
pca = PCA(n_components=0.95)  # Retain 95% of variance
X_pca_train = pca.fit_transform(new_X_train)
X_pca_test = pca.transform(new_X_test)

# Initialize models and storage
classification_models = {
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Support Vector Machine": SVC(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "Gaussian Naive Bayes": GaussianNB(),
    "XGBoost": XGBClassifier(),
}

model_names = []
accuracies = []

# Train and evaluate each model on PCA-transformed data
for name, clf in classification_models.items():
    clf.fit(X_pca_train, new_y_train)
    y_pred = clf.predict(X_pca_test)
    score = accuracy_score(new_y_test, y_pred)
    model_names.append(name)
    accuracies.append(score)
    print(f"{name} accuracy after PCA: {score:.2f}")

# Create a DataFrame for model accuracies
new_df_models = pd.DataFrame({'Model': model_names, 'Accuracy': accuracies})

# Display the model accuracies
print(new_df_models)

# Convert accuracies to percentages (with 2 decimal places)
new_df_models['Accuracy (%)'] = (new_df_models['Accuracy'] * 100).round(2)

# Plot model accuracies using Plotly and show the accuracy on top of each bar
fig = px.bar(new_df_models, x='Model', y='Accuracy', title='Model Accuracies after PCA',
             text='Accuracy (%)')

# Customize the text position and format
fig.update_traces(textposition='outside')

# Update the y-axis to show percentage (if desired)
fig.update_layout(yaxis_tickformat='%')

# Show the plot
fig.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report

# Initialize lists to store results
model_names = []
accuracies = []
precisions = []
recalls = []
f1_scores = []
roc_aucs = []

# Modify SVC to include probability
classification_models["Support Vector Machine"] = SVC(probability=True)

# Evaluate each model
for name, clf in classification_models.items():
    clf.fit(new_X_train, new_y_train)
    y_pred = clf.predict(new_X_test)

    # Calculate metrics
    accuracy = accuracy_score(new_y_test, y_pred)
    precision = precision_score(new_y_test, y_pred, average='weighted')
    recall = recall_score(new_y_test, y_pred, average='weighted')
    f1 = f1_score(new_y_test, y_pred, average='weighted')

    # Calculate ROC-AUC if possible
    try:
        if len(set(new_y_test)) == 2:  # Binary classification
            roc_auc = roc_auc_score(new_y_test, clf.predict_proba(new_X_test)[:, 1])
        else:  # Multiclass
            roc_auc = roc_auc_score(new_y_test, clf.predict_proba(new_X_test), multi_class='ovr')
    except AttributeError:
        roc_auc = None  # Set to None if `predict_proba` isn't available

    # Append results to lists
    model_names.append(name)
    accuracies.append(accuracy)
    precisions.append(precision)
    recalls.append(recall)
    f1_scores.append(f1)
    roc_aucs.append(roc_auc)

    # Print classification report for each model
    print(f"Classification Report for {name}:\n", classification_report(new_y_test, y_pred))

# Create DataFrame for easy comparison
metrics_df = pd.DataFrame({
    'Model': model_names,
    'Accuracy': accuracies,
    'Precision': precisions,
    'Recall': recalls,
    'F1 Score': f1_scores,
    'ROC-AUC': roc_aucs
})

# Display the metrics DataFrame
print(metrics_df)

# Sample Data (assuming `metrics_df` is your DataFrame)
metrics_df = pd.DataFrame({
    'Model': ['K-Nearest Neighbors', 'Support Vector Machine', 'Logistic Regression', 'Decision Tree',
              'Random Forest', 'Gradient Boosting', 'AdaBoost', 'Gaussian Naive Bayes', 'XGBoost'],
    'Accuracy': [0.71, 0.80, 0.75, 0.84, 0.91, 0.90, 0.88, 0.76, 0.91],
    'Precision': [0.69, 0.80, 0.71, 0.84, 0.91, 0.90, 0.89, 0.73, 0.91],
    'Recall': [0.71, 0.80, 0.75, 0.84, 0.91, 0.90, 0.89, 0.76, 0.91],
    'F1 Score': [0.69, 0.78, 0.73, 0.84, 0.91, 0.90, 0.88, 0.74, 0.91]
})

# Set figure size
plt.figure(figsize=(20, 10))

# Set bar width and positions
bar_width = 0.18
index = np.arange(len(metrics_df))

# Define colors for each metric
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

# Plot each metric with model names as x-ticks
plt.bar(index, metrics_df['Accuracy'], bar_width, label='Accuracy', color=colors[0])
plt.bar(index + bar_width, metrics_df['Precision'], bar_width, label='Precision', color=colors[1])
plt.bar(index + 2 * bar_width, metrics_df['Recall'], bar_width, label='Recall', color=colors[2])
plt.bar(index + 3 * bar_width, metrics_df['F1 Score'], bar_width, label='F1 Score', color=colors[3])

# Add scores above each bar
for i, metric in enumerate(['Accuracy', 'Precision', 'Recall', 'F1 Score']):
    for j, value in enumerate(metrics_df[metric]):
        plt.text(j + i * bar_width, value + 0.005, f"{value:.2f}", ha='center', va='bottom', fontsize=10)

# Add labels, title, and improve font sizes
plt.xlabel('Model', fontsize=14)
plt.ylabel('Scores', fontsize=14)
plt.title('Model Performance Comparison', fontsize=16)
plt.xticks(index + 1.5 * bar_width, metrics_df['Model'], rotation=45, ha='right', fontsize=12)
plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fontsize=12)

plt.tight_layout()
plt.show()

"""ROC & AUC plot"""

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Binarize the output if it's multiclass
classes = np.unique(new_y_test)
if len(classes) > 2:
    y_test_binarized = label_binarize(new_y_test, classes=classes)
else:
    y_test_binarized = new_y_test

# Loop through each model to create a separate plot
for name, model in classification_models.items():
    model.fit(new_X_train, new_y_train)

    # Check if the model has probability scores or decision function
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(new_X_test)
    elif hasattr(model, "decision_function"):
        y_score = model.decision_function(new_X_test)
    else:
        continue  # Skip models without `predict_proba` or `decision_function`

    plt.figure(figsize=(8, 6))

    # Calculate ROC curve and ROC-AUC for each class
    if len(classes) == 2:  # Binary classification
        fpr, tpr, _ = roc_curve(new_y_test, y_score[:, 1])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"Class 1 (AUC = {roc_auc:.2f})")
    else:  # Multiclass
        for i in range(len(classes)):
            fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_score[:, i])
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f"Class {classes[i]} (AUC = {roc_auc:.2f})")

    # Plot diagonal line for random guessing
    plt.plot([0, 1], [0, 1], 'k--', label="Chance (AUC = 0.5)")

    # Add labels and title
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title(f"ROC-AUC Curve for {name}")
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)

    # Show plot for the current model
    plt.show()

Y = new_df['GradeClass' ]
X = new_df.drop(columns=['GradeClass' , 'StudentID', 'Volunteering', 'Sports', 'Music'] , axis=1)
new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(X, Y, test_size=0.2, random_state=42)


# Hyperparameter tuning for Random Forest
rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

rf_grid_search = GridSearchCV(RandomForestClassifier(), param_grid=rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
rf_grid_search.fit(new_X_train, new_y_train)

# Best Random Forest parameters and accuracy
best_rf_params = rf_grid_search.best_params_
best_rf_accuracy = rf_grid_search.best_score_

print(f"Best Random Forest Parameters: {best_rf_params}")
print(f"Best Random Forest Cross-Validated Accuracy: {best_rf_accuracy:.4f}")

# Evaluate the best Random Forest model on the test set
best_rf_model = rf_grid_search.best_estimator_
rf_test_accuracy = accuracy_score(new_y_test, best_rf_model.predict(new_X_test))
print(f"Random Forest Test Accuracy: {rf_test_accuracy:.4f}")

# Hyperparameter tuning for XGBoost
xgb_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'gamma': [0, 0.1, 0.5] #regularization
}


# Best XGBoost parameters and accuracy
best_xgb_params = xgb_grid_search.best_params_
best_xgb_accuracy = xgb_grid_search.best_score_

print(f"Best XGBoost Parameters: {best_xgb_params}")
print(f"Best XGBoost Cross-Validated Accuracy: {best_xgb_accuracy:.4f}")

# Evaluate the best XGBoost model on the test set
best_xgb_model = xgb_grid_search.best_estimator_
xgb_test_accuracy = accuracy_score(new_y_test, best_xgb_model.predict(new_X_test))
print(f"XGBoost Test Accuracy: {xgb_test_accuracy:.4f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Refit models with best parameters
best_rf = rf_grid_search.best_estimator_
best_rf.fit(new_X_train, new_y_train)
y_pred_rf = best_rf.predict(new_X_test)

best_xgb = xgb_grid_search.best_estimator_
best_xgb.fit(new_X_train, new_y_train)
y_pred_xgb = best_xgb.predict(new_X_test)

# Plot confusion matrices
plt.figure(figsize=(14, 6))

# Confusion matrix for Random Forest
plt.subplot(1, 2, 1)
cm_rf = confusion_matrix(new_y_test, y_pred_rf)
ConfusionMatrixDisplay(cm_rf).plot(cmap="Blues", ax=plt.gca())
plt.title("Confusion Matrix - Random Forest")

# Confusion matrix for XGBoost
plt.subplot(1, 2, 2)
cm_xgb = confusion_matrix(new_y_test, y_pred_xgb)
ConfusionMatrixDisplay(cm_xgb).plot(cmap="Blues", ax=plt.gca())
plt.title("Confusion Matrix - XGBoost")

plt.show()

df
scale = StandardScaler()
X = df.drop(['GradeClass' ,'StudentID','Sports', 'Music'], axis=1)
y = df['GradeClass']
scale.fit_transform(X)

X_train, X_, y_train, y_ = train_test_split(X, y, test_size=0.2, random_state=123)
X_test , X_CV , y_test , y_CV = train_test_split(X_, y_, test_size=0.5, random_state=123)

X_CV.describe()

# Define models
classification_models = {
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Support Vector Machine": SVC(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "Gaussian Naive Bayes": GaussianNB(),
    "XGBoost": XGBClassifier()
}

# Lists to store results
model_names = []
test_accuracies = []
cv_accuracies = []

# Split the data
X_train, X_CV, y_train, y_CV = train_test_split(X, y, test_size=0.2, random_state=42)
X_CV, X_test, y_CV, y_test = train_test_split(X_CV, y_CV, test_size=0.5, random_state=42)

# Train and evaluate each model
for name, clf in classification_models.items():
    clf.fit(X_train, y_train)

    # Accuracy on test set
    y_pred_test = clf.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred_test)
    test_accuracies.append(test_accuracy)

    # Cross-validation accuracy
    y_pred_cv = clf.predict(X_CV)
    cv_accuracy = accuracy_score(y_CV, y_pred_cv)
    cv_accuracies.append(cv_accuracy)

    # Store the model name
    model_names.append(name)

    print(f"{name} - Test Accuracy: {test_accuracy:.2f}, Cross-Validation Accuracy: {cv_accuracy:.2f}")

# Create a DataFrame for the accuracies
results_df = pd.DataFrame({
    'Model': model_names,
    'Test Accuracy': test_accuracies,
    'Cross-validation Accuracy': cv_accuracies
})

# Convert accuracies to percentages for the bar plot
results_df['Test Accuracy (%)'] = (results_df['Test Accuracy'] * 100).round(2)
results_df['Cross-validation Accuracy (%)'] = (results_df['Cross-validation Accuracy'] * 100).round(2)

# Plot both Test Accuracy and Cross-validation Accuracy using Plotly
fig = px.bar(results_df, x='Model', y=['Test Accuracy', 'Cross-validation Accuracy'],
             title='Model Test and Cross-Validation Accuracies', barmode='group')

# Customize the text position and format
fig.update_traces(texttemplate='%{value:.2f}', textposition='outside')

# Show the plot
fig.show()

# Display the DataFrame with results
print(results_df)